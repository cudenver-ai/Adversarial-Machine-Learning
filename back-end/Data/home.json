[
    {
    "logo": "https://picsum.photos/100/100?random=101",
    "name": "Challenge Overview",
    "description": "**Challenge Overview**\n\nWe are excited to introduce the **Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI**, part of the **CU Denver Data Science and AI Symposium**. \n\n_Do you see a gibbon in the above picture? We can make the image classifier make this prediction with imperceptible modifications. Ref. I. J. Goodfellow, et. Al., \"Explaining and harnessing adversarial examples\" 2014._\n\nThis challenge invites **all CU Denver students** to dive into the intriguing world of **adversarial machine learning** by crafting adversarial examples that can deceive a robust machine learning classifier trained on the **CIFAR-10 dataset**. Your mission is to create subtle but effective modifications to a set of test images, fooling the classifier into making incorrect predictions. This challenge is a perfect opportunity for students to explore model vulnerabilities, gain hands-on experience with adversarial techniques, and contribute to ongoing research in AI robustness and security.\n\n**Introduction**\n\nThe CIFAR-10 dataset is a well-known benchmark for image classification tasks, consisting of 10 classes such as airplanes, dogs, and ships. Although deep learning models have achieved impressive accuracy on this dataset, they remain vulnerable to **adversarial examples**—inputs that have been carefully manipulated to mislead models while appearing almost identical to the human eye.\n\nIn the **Decoy Challenge**, CU Denver students will have the chance to exploit these vulnerabilities. You will be provided with a **robust classifier** trained on CIFAR-10 and a set of test images. Your task is to generate small perturbations to these test images that fool the classifier, forcing it to make incorrect predictions. By participating, you’ll be engaging in cutting-edge research that explores the limits of AI systems and helps to improve future defenses against adversarial attacks.\n\nThis challenge offers a unique opportunity to apply theoretical knowledge in a practical setting, gain exposure to adversarial machine learning techniques, and contribute to building more robust AI systems that can better withstand adversarial attacks in real-world applications.\n\n**The Challenge**\n\nThe **Decoy Challenge** focuses on generating adversarial examples that can mislead a machine learning model trained on the CIFAR-10 dataset. Participants will receive a pre-trained, **robust classifier** and a set of test examples from the CIFAR-10 dataset. Your objective is to create subtle perturbations to these test examples that can fool the classifier while maintaining the images' visual integrity.\n\n**Types of Perturbations**\n\nThe adversarial examples you generate should be crafted using small, imperceptible perturbations to the test images. These perturbations should not significantly change the visual appearance of the images but must be sufficient to cause the classifier to misclassify them.\n\n**Objective**\n\n*   Your goal is to **maximize the classifier’s error rate** by making slight modifications to the provided test images.\n    \n*   The challenge will test how effectively you can deceive a robust classifier while keeping the perturbations small and unnoticeable to the human eye.\n    \n\n**Specific Tasks:**\n\n1.  **Adversarial Example Generation \\[Classification, Deception\\]**:\n    \n    *   Participants will receive a set of CIFAR-10 test images and a pre-trained robust classifier.\n        \n    *   The task is to modify these test images so that the classifier makes incorrect predictions on as many images as possible.\n        \n2.  **Perturbation Constraints**:\n    \n    *   The perturbations should be small in magnitude, ensuring the modified images remain visually similar to the originals.\n        \n    *   Participants will be evaluated on their ability to create **efficient adversarial examples**—those that confuse the classifier with minimal visible changes to the input images.\n        \n\nBy taking on the **Decoy Challenge**, you will not only test your skills in adversarial machine learning but also contribute to the ongoing research aimed at creating more resilient AI models. Join us in advancing the understanding of adversarial techniques and their impact on machine learning!\n\n**Rules and Evaluation**\n\n**Rules**\n\nTo participate in the **Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI**, all CU Denver students must follow these guidelines to ensure a fair and streamlined competition:\n\n1.  **Dataset and Classifier**:\n    \n    *   You will be provided with a **pre-trained CIFAR-10 classifier** and a **test set** of images.\n        \n    *   You are free to use **any tools** or **state-of-the-art adversarial attack algorithms** (e.g., FGSM, PGD, C&W, etc.) to generate adversarial examples.\n        \n    *   There are **no specific constraints on perturbation magnitude**—your goal is to successfully deceive the classifier, but subtle, effective attacks will likely score higher.\n        \n2.  **Submission**:\n    \n    *   Participants are required to submit only the **perturbed versions of the test images**.\n        \n        *   The perturbed data must be submitted in a **pickle file format** as described in the provided starter code.\n            \n    *   Each submission should include:\n        \n        *   A **pickle file** containing the adversarial examples.\n            \n        *   Submit the code along with a **README** describing the method used to generate the adversarial examples.\n            \n        *   Visual examples comparing original and perturbed images are _**optional**_ but recommended for clarity.\n            \n    *   The **starter code** provided will help you understand how to format your submission and implement various adversarial algorithms.\n        \n3.  **Evaluation Time Limits**:\n    \n    *   Your adversarial examples should be generated within a **reasonable time frame** using standard tools and techniques.\n        \n4.  **Ethics**:\n    \n    *   All participants must ensure their submissions adhere to ethical standards. Any inappropriate content will result in disqualification.\n        \n5.  **Final Submission**:\n    \n    *   The final submission deadline is **October 29th, 2024**. Late submissions will not be accepted.\n        \n\n**Evaluation**\n\nYour submission will be evaluated based on a **multi-faceted scoring system** that balances adversarial effectiveness, model confidence, and the degree of perturbation and perceptual similarity. The scoring formula is as follows:\n\nutility measures add up to 0.5\n\n**Explanation of Metrics:**\n\n1.  **Incorrect Classification Ratio (incorrect\\_ratio)**:\n    \n    *   The proportion of adversarial examples that cause the classifier to make **incorrect predictions**.\n        \n    *   A higher incorrect ratio means a more successful attack.\n        \n2.  **Confidence on Incorrect Predictions (avg\\_confidence\\_incorrect)**:\n    \n    *   Measures the classifier’s **confidence** in the incorrect predictions made after the adversarial perturbations.\n        \n    *   Higher confidence in incorrect predictions will improve your score.\n        \n3.  **Confidence Gap (avg\\_confidence\\_gap)**:\n    \n    *   Compares the **confidence drop** between the correct prediction (before perturbation) and the incorrect prediction (after perturbation).\n        \n    *   A higher confidence gap indicates a more significant attack impact.\n        \n4.  **Perturbation Magnitude (L2 norm) (avg\\_l2\\_perturbation)**:\n    \n    *   Quantifies the **average size** of the perturbation. Lower perturbations are generally preferred.\n        \n    *   This is normalized by the maximum possible perturbation to ensure fairness across different approaches.\n        \n5.  **Perceptual Similarity (SSIM) (avg\\_ssim)**:\n    \n    *   Evaluates how visually similar the perturbed images are to the original ones.\n        \n    *   Lower SSIM values (closer to 0) indicate a more successful attack since it implies that the perturbation was perceptually subtle yet effective.\n        \n\n**Timing and Duration**\n\nThe **Decoy Challenge** will take place from **October 1st, 2024**, to **October 29th, 2024**. During this period, participants will have access to the test dataset and classifier, along with the starter code, to develop their adversarial examples.\n\n**Key Dates:**\n\n*   **Challenge Start**: October 1st, 2024\n    \n*   **Submission Deadline**: October 29th, 2024, 11:59 PM (CET)\n    \n*   **CU Denver Data Science and AI Symposium**: November 1st, 2024\n    \n    *   Winners will be notified on October 30th, and their solutions presented during the symposium.\n        \n\n**Participation**\n\nThe **Decoy Challenge** is open to **all CU Denver students** across disciplines, interested in data science, Aritificial Intelligence, and related fields. Whether you’re a beginner or an advanced student, this challenge allows you to explore adversarial machine learning in a practical setting.\n\n**How to Participate:**\n\n1.  **Register** for the challenge through the AI Student Association.\n    \n2.  Download the provided **test set** and **classifier**. \n    \n3.  Use any tools or algorithms to generate adversarial examples and submit your **perturbed data** in the format specified by the starter code.\n    \n\nYou can participate **individually** or in **teams** (**up to 4 members**). This is your opportunity to test your skills, learn new techniques, and compete with fellow students in advancing adversarial machine learning.\n\nGood luck, and let’s see who can outsmart AI!\n\n**Prize**\n\nThe **Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI** offers exciting opportunities for CU Denver students to showcase their skills in adversarial machine learning. The top-performing teams will not only receive **monetary rewards** but will also have the prestigious opportunity to present their work at the **CU Denver Data Science and AI Symposium** on **November 1st, 2024**.\n\n**Prizes for the Top 3 Teams:**\n\n1.  **1st Place**:\n    \n    *   **Monetary Prize**: Awarded to the team with the highest overall score in the challenge.\n        \n    *   **Presentation Opportunity**: The first-place team will present their solution and methods to an audience of AI researchers, data scientists, and industry professionals at the CU Denver Data Science and AI Symposium.\n        \n2.  **2nd Place**:\n    \n    *   **Monetary Prize**: Awarded to the team with the second-highest score.\n        \n    *   **Presentation Opportunity**: The second-place team will also have the chance to showcase their approach during the symposium.\n        \n3.  **3rd Place**:\n    \n    *   **Monetary Prize**: Awarded to the team with the third-highest score.\n        \n    *   **Presentation Opportunity**: The third-place team will present their adversarial techniques at the symposium.\n        \n4.  **Top 10 Teams**: \n    \n    *   **Certificate Prize:** Awarded to the top 5 teams certificate of participation.\n        \n\n**Recognition:**\n\nAll winners will receive recognition during the symposium and be featured in CU Denver’s AI and Data Science community, providing valuable exposure and networking opportunities.\n\nWhether you're looking to hone your skills, gain recognition, or win exciting prizes, this challenge offers CU Denver students a unique chance to make their mark in the field of adversarial machine learning!",
    "captain": "We are excited to introduce the Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI, part of the CU Denver Data Science and AI Symposium."
  }
]