[
  {
    "img": "https://picsum.photos/800/450?image=442",
    "title": "Challenge Objective",
    "description": "The objective of this challenge is to create adversarial examples that can mislead a robust AI model. Participants must craft subtle perturbations that deceive the classifier while preserving image quality."
  },
  {
    "title": "Submission Guidelines",
    "description": "Submit the adversarial examples in the required format. Include a detailed README outlining the methods and algorithms used. Visual comparisons of the original and perturbed images are encouraged."
  },
  {
    "title": "Rules & Guidelines",
    "description": [
      "1) Submissions must adhere to the provided format.",
      "2) Any type of injection attack will be logged and reported.",
      "3) Each team is allowed one per hour.",
      "4) Follow school code of conduct at all times."
    ]
  },
  {
    "img": "https://picsum.photos/800/450?image=692",
    "tag": "Dataset Description",
    "title": "Dataset Description",
    "description": "Participants will use the CIFAR-10 dataset and a pre-trained classifier. Use state-of-the-art adversarial techniques to craft perturbations that successfully deceive the model without compromising image integrity."
  }
]
