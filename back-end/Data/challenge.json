{
    "title": "Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI",
    "intro": "We are excited to introduce the Decoy Challenge: Deceptive Examples to Confuse and Outsmart Your AI, part of the CU Denver Data Science and AI Symposium.",
    "content": [
      {
        "section": "Introduction",
        "text": "The CIFAR-10 dataset is a well-known benchmark for image classification tasks, consisting of 10 classes such as airplanes, dogs, and ships. Although deep learning models have achieved impressive accuracy on this dataset, they remain vulnerable to adversarial examples—inputs that have been carefully manipulated to mislead models while appearing almost identical to the human eye."
      },
      {
        "section": "The Challenge",
        "text": "The Decoy Challenge focuses on generating adversarial examples that can mislead a machine learning model trained on the CIFAR-10 dataset. Participants will receive a pre-trained, robust classifier and a set of test examples from the CIFAR-10 dataset. Your objective is to create subtle perturbations to these test examples that can fool the classifier while maintaining the images' visual integrity."
      },
      {
        "section": "Types of Perturbations",
        "text": "The adversarial examples you generate should be crafted using small, imperceptible perturbations to the test images. These perturbations should not significantly change the visual appearance of the images but must be sufficient to cause the classifier to misclassify them."
      },
      {
        "section": "Objective",
        "text": "Your goal is to maximize the classifier’s error rate by making slight modifications to the provided test images. The challenge will test how effectively you can deceive a robust classifier while keeping the perturbations small and unnoticeable to the human eye."
      },
      {
        "section": "Specific Tasks",
        "text": "Adversarial Example Generation, Perturbation Constraints, Evaluation Criteria, and Tools and Constraints are provided in the challenge description."
      },
      {
        "section": "Submission Requirements",
        "text": "Participants must submit a Jupyter Notebook containing the code used to generate adversarial examples, evaluation results showing the classifier’s misclassification rate, and visual comparisons between original and perturbed images."
      },
      {
        "section": "Rules",
        "text": "The adversarial examples must only be generated from the provided test dataset. Your submission should include code for evaluating the performance of your perturbations."
      }
    ],
    "dataset": "Dataset Description",
    "rules": [
      "1) Rule 1",
      "2) Rule 2",
      "3) Rule 3",
      "4) Rule 4"
    ],
    "objective": "The Decoy Challenge focuses on generating adversarial examples that can mislead a machine learning model trained on the CIFAR-10 dataset. Participants will receive a pre-trained, robust classifier and a set of test examples from the CIFAR-10 dataset. Your objective is to create subtle perturbations to these test examples that can fool the classifier while maintaining the images' visual integrity.",
    "submission": "Submission Requirements",
    "eval": "Evaluation Criteria"
  }
